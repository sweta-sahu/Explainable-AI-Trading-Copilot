{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38cf318c-50a6-4737-95a7-935f206c7ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: s3://ai-trading-copilot-curated/features/fe-yearly-run-2025-10-17-23-39-01/AAPL_features.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/fsspec/registry.py:298: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before dropping NaNs: (752, 25)\n",
      "Shape after dropping NaNs: (752, 25)\n",
      "\n",
      "✅ Your test period starts on: 2025-03-12\n",
      "✅ Your test period ends on:   2025-10-15\n",
      "\n",
      "Choose any trading date within this range for your TEST_DATE_STR.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime \n",
    "\n",
    "# --- Configuration ---\n",
    "FEATURE_FILE = \"s3://ai-trading-copilot-curated/features/fe-yearly-run-2025-10-17-23-39-01/AAPL_features.csv\" \n",
    "\n",
    "# --- Load and Prepare ---\n",
    "try:\n",
    "    print(f\"Loading data from: {FEATURE_FILE}\")\n",
    "    df = pd.read_csv(FEATURE_FILE)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values(by='date')\n",
    "\n",
    "    # --- Replicate the NaN drop logic from feature_builder.py ---\n",
    "    # Define features that might have NaNs at the beginning due to rolling/lag\n",
    "    features_to_check_for_nans = [\n",
    "        'ret_1d', 'mom_5d', 'rsi_14', 'abn_volume',\n",
    "        'ret_lag_1d', 'volatility_20d', 'ma_50d', 'ma_200d',\n",
    "        'return_x_volume'\n",
    "     ]\n",
    "\n",
    "    print(f\"Shape before dropping NaNs: {df.shape}\")\n",
    "    df_cleaned = df.dropna(subset=features_to_check_for_nans)\n",
    "    print(f\"Shape after dropping NaNs: {df_cleaned.shape}\")\n",
    "\n",
    "    # Perform the 80/20 split on the cleaned data\n",
    "    split_index = int(len(df_cleaned) * 0.8)\n",
    "    test_df = df_cleaned[split_index:]\n",
    "\n",
    "    # --- Print the Test Period ---\n",
    "    if not test_df.empty:\n",
    "        start_test_date = test_df['date'].min().date()\n",
    "        end_test_date = test_df['date'].max().date()\n",
    "\n",
    "        print(f\"\\n Your test period starts on: {start_test_date}\")\n",
    "        print(f\" Your test period ends on:   {end_test_date}\")\n",
    "        print(\"\\nChoose any trading date within this range for your TEST_DATE_STR.\")\n",
    "    else:\n",
    "        print(\" ERROR: Test dataframe is empty after splitting. Check NaN handling or data.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\" ERROR: Feature file not found at '{FEATURE_FILE}'. Please update the path.\")\n",
    "except KeyError as e:\n",
    "    print(f\" ERROR: A required column is missing from the CSV file: {e}\")\n",
    "except Exception as e:\n",
    "     print(f\" ERROR: Could not process file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd8d5277-0cb2-4450-8fb7-3d6c1531dfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: model.pkl\n",
      "[22:09:47] WARNING: ../src/learner.cc:553: \n",
      "  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n",
      "  older XGBoost, please export the model by calling `Booster.save_model` from that version\n",
      "  first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.\n",
      "\n",
      "Loading features from: s3://ai-trading-copilot-curated/features/fe-yearly-run-2025-10-17-23-39-01/AAPL_features.csv\n",
      "\n",
      "--- ✅ Historical Test Complete for AAPL on 2025-09-15 ---\n",
      "  Features used from: 2025-09-15\n",
      "  Model Predicted:    Up (Confidence: 52.55%)\n",
      "  Actual Outcome was: Up (for day 2025-09-15's close vs next day's close)\n",
      "  ✅ Prediction was CORRECT!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_PATH = \"model.pkl\"\n",
    "FEATURE_FILE = \"s3://ai-trading-copilot-curated/features/fe-yearly-run-2025-10-17-23-39-01/AAPL_features.csv\"\n",
    "TICKER_SYMBOL = 'AAPL'\n",
    "TEST_DATE_STR = \"2025-09-15\" \n",
    "\n",
    "def test_model_on_date(ticker_symbol, feature_file_path, model_path, test_date_str):\n",
    "    \"\"\"Loads model, gets features for a specific date, predicts, and compares to actual.\"\"\"\n",
    "\n",
    "    # --- 1. Load Model ---\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    try:\n",
    "        model = joblib.load(model_path)\n",
    "    except Exception as e:\n",
    "        print(f\" ERROR: Could not load model: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Load Full Feature Data ---\n",
    "    print(f\"Loading features from: {feature_file_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(feature_file_path)\n",
    "        df['date'] = pd.to_datetime(df['date']) \n",
    "    except Exception as e:\n",
    "        print(f\" ERROR: Could not read feature file: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Create Target  ---\n",
    "    df['target'] = (df.groupby('symbol')['close'].shift(-1) > df['close']).astype(int)\n",
    "\n",
    "    # --- 4. Select the Specific Test Day's Data ---\n",
    "    try:\n",
    "        test_date = datetime.strptime(test_date_str, \"%Y-%m-%d\").date()\n",
    "        # Get the row corresponding to the chosen date\n",
    "        day_data = df[df['date'].dt.date == test_date].copy()\n",
    "    except ValueError:\n",
    "        print(f\" ERROR: Invalid date format '{test_date_str}'. Use YYYY-MM-DD.\")\n",
    "        return\n",
    "\n",
    "    if day_data.empty:\n",
    "        print(f\" ERROR: No data found for {ticker_symbol} on {test_date_str}. Was it a trading day?\")\n",
    "        return\n",
    "\n",
    "    # --- 5. Get Features and Known Outcome ---\n",
    "    known_non_features = ['date', 'open', 'high', 'low', 'close', 'volume', 'symbol', 'asof', 'target']\n",
    "    features = [col for col in df.columns if col not in known_non_features]\n",
    "\n",
    "    # Check for NaNs in features for this specific day (might happen with lags/rolling)\n",
    "    if day_data[features].isnull().values.any():\n",
    "        print(f\" WARNING: The data for {test_date_str} contains NaN values in features. Prediction might be unreliable or fail.\")\n",
    "        print(day_data[features].isnull().sum())\n",
    "        # Decide if you want to proceed or stop\n",
    "        # return\n",
    "\n",
    "    X_test_day = day_data[features]\n",
    "    # Get the pre-calculated target value for this day (outcome of the NEXT day)\n",
    "    known_outcome = day_data['target'].iloc[0] # .iloc[0] gets the value from the single row\n",
    "\n",
    "    # --- 6. Make Prediction ---\n",
    "    try:\n",
    "        prediction = model.predict(X_test_day)[0]\n",
    "        prediction_proba = model.predict_proba(X_test_day)[0]\n",
    "    except Exception as e:\n",
    "        print(f\" ERROR: Prediction failed for {test_date_str}: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 7. Compare and Display Results ---\n",
    "    predicted_direction = \"Up\" if prediction == 1 else \"Down\"\n",
    "    actual_direction = \"Up\" if known_outcome == 1 else \"Down\"\n",
    "    confidence = prediction_proba[1] if predicted_direction == \"Up\" else prediction_proba[0]\n",
    "\n",
    "    print(f\"\\n--- Historical Test Complete for {ticker_symbol} on {test_date_str} ---\")\n",
    "    print(f\"  Features used from: {test_date_str}\")\n",
    "    print(f\"  Model Predicted:    {predicted_direction} (Confidence: {confidence:.2%})\")\n",
    "    print(f\"  Actual Outcome was: {actual_direction} (for day {test_date_str}'s close vs next day's close)\")\n",
    "\n",
    "    if predicted_direction == actual_direction:\n",
    "        print(\" Prediction was CORRECT!\")\n",
    "    else:\n",
    "        print(\" Prediction was INCORRECT.\")\n",
    "\n",
    "# --- Run the test ---\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "         print(f\"Please ensure 'model.pkl' is in the correct path: '{MODEL_PATH}'.\")\n",
    "    else:\n",
    "        test_model_on_date(TICKER_SYMBOL, FEATURE_FILE, MODEL_PATH, TEST_DATE_STR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98a0c9a0-2852-472b-9b33-f762d6c67899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: model.pkl\n",
      "[22:10:23] WARNING: ../src/learner.cc:553: \n",
      "  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n",
      "  older XGBoost, please export the model by calling `Booster.save_model` from that version\n",
      "  first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.\n",
      "\n",
      "Loading all feature files from: s3://ai-trading-copilot-curated/features/fe-yearly-run-2025-10-17-23-39-01/\n",
      "Loaded combined data shape: (2256, 25)\n",
      "Shape after NaN drop: (2256, 26)\n",
      "Test set size: 452 rows\n",
      "Making predictions on the test set...\n",
      "\n",
      "--- ✅ Overall Test Set Evaluation ---\n",
      "  Overall Accuracy: 0.6305 (63.05%)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Down (0)       0.67      0.38      0.48       207\n",
      "      Up (1)       0.62      0.84      0.71       245\n",
      "\n",
      "    accuracy                           0.63       452\n",
      "   macro avg       0.64      0.61      0.60       452\n",
      "weighted avg       0.64      0.63      0.61       452\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import sagemaker # Need sagemaker session to easily list S3 files\n",
    "import boto3\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_PATH = \"model.pkl\" \n",
    "S3_FEATURE_PREFIX = \"s3://ai-trading-copilot-curated/features/fe-yearly-run-2025-10-17-23-39-01/\" \n",
    "\n",
    "def calculate_overall_accuracy(model_path, s3_feature_prefix):\n",
    "    \"\"\"Loads the model and calculates accuracy over the entire test set.\"\"\"\n",
    "\n",
    "    # --- 1. Load Local Model ---\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    try:\n",
    "        model = joblib.load(model_path)\n",
    "    except Exception as e:\n",
    "        print(f\" ERROR: Could not load model: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Load ALL Feature Data from S3 ---\n",
    "    print(f\"Loading all feature files from: {s3_feature_prefix}\")\n",
    "    all_features_df = pd.DataFrame()\n",
    "    try:\n",
    "        # Use sagemaker session or boto3 to list files in the S3 prefix\n",
    "        s3_uri_parts = urlparse(s3_feature_prefix)\n",
    "        bucket = s3_uri_parts.netloc\n",
    "        prefix = s3_uri_parts.path.lstrip('/')\n",
    "\n",
    "        s3_client = boto3.client('s3')\n",
    "        paginator = s3_client.get_paginator('list_objects_v2')\n",
    "        pages = paginator.paginate(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "        feature_files_s3 = []\n",
    "        for page in pages:\n",
    "            if \"Contents\" in page:\n",
    "                for obj in page['Contents']:\n",
    "                    key = obj['Key']\n",
    "                    if key.endswith('_features.csv'):\n",
    "                         feature_files_s3.append(f\"s3://{bucket}/{key}\")\n",
    "\n",
    "        if not feature_files_s3:\n",
    "            print(f\" ERROR: No feature files found at {s3_feature_prefix}\")\n",
    "            return\n",
    "\n",
    "        # Read and concatenate all files\n",
    "        all_features_df = pd.concat((pd.read_csv(f) for f in feature_files_s3), ignore_index=True)\n",
    "        all_features_df['date'] = pd.to_datetime(all_features_df['date'])\n",
    "        all_features_df = all_features_df.sort_values(by=['date', 'symbol']).reset_index(drop=True)\n",
    "        print(f\"Loaded combined data shape: {all_features_df.shape}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" ERROR: Could not read feature files from S3: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Training Preprocessing ---\n",
    "    # Create target\n",
    "    all_features_df['target'] = (all_features_df.groupby('symbol')['close'].shift(-1) > all_features_df['close']).astype(int)\n",
    "\n",
    "    # Define features \n",
    "    known_non_features = ['date', 'open', 'high', 'low', 'close', 'volume', 'symbol', 'asof', 'target']\n",
    "    features = [col for col in all_features_df.columns if col not in known_non_features]\n",
    "\n",
    "    # Define features used for dropping NaNs\n",
    "    features_to_check_for_nans = [\n",
    "        'ret_1d', 'mom_5d', 'rsi_14', 'abn_volume',\n",
    "        'ret_lag_1d', 'volatility_20d', 'ma_50d', 'ma_200d',\n",
    "        'return_x_volume'\n",
    "     ]\n",
    "    # Also include the target for dropping the last row of each symbol\n",
    "    final_df = all_features_df.dropna(subset=features_to_check_for_nans + ['target'])\n",
    "    print(f\"Shape after NaN drop: {final_df.shape}\")\n",
    "\n",
    "\n",
    "    # --- 4. Perform 80/20 Chronological Split ---\n",
    "    X = final_df[features]\n",
    "    y = final_df['target']\n",
    "    split_index = int(len(final_df) * 0.8)\n",
    "    X_train, X_test = X[:split_index], X[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "    if X_test.empty:\n",
    "        print(\" ERROR: Test set is empty after splitting. Check data or split logic.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Test set size: {len(X_test)} rows\")\n",
    "\n",
    "    # --- 5. Make Predictions on Test Set ---\n",
    "    print(\"Making predictions on the test set...\")\n",
    "    try:\n",
    "        predictions = model.predict(X_test)\n",
    "    except Exception as e:\n",
    "        print(f\" ERROR: Prediction failed on test set: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 6. Calculate and Display Metrics ---\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "    print(\"\\n--- Overall Test Set Evaluation ---\")\n",
    "    print(f\"  Overall Accuracy: {accuracy:.4f} ({accuracy:.2%})\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, predictions, target_names=['Down (0)', 'Up (1)']))\n",
    "\n",
    "# --- Run the calculation ---\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(f\"Please ensure 'model.pkl' is at '{MODEL_PATH}'.\")\n",
    "    else:\n",
    "        calculate_overall_accuracy(MODEL_PATH, S3_FEATURE_PREFIX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
